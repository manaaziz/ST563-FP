---
title: "ST563 Final Project"
author:
  - Final Project Group 6
  - Mana Azizsoltani, Rong Huang, Yun Ji, Jackie Steffan
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document:
    toc: true
    toc_depth: 1
fontsize: 12pt
---

```{r setup, include=FALSE}
# Set default chunk options
knitr::opts_chunk$set(echo = FALSE, message = FALSE, cache = FALSE,
                      warning = FALSE, fig.align = "center",
                      out.height = '3.5in')

# Load necessary libraries
library(tidyverse) # so many things
library(knitr) # making nice tables
library(leaps) # subset selection
library(corrplot) # visualize correlation
library(rmarkdown) # for output documents
library(randomForest) # fitting random forest model
library(kableExtra) # making bolded headers for tables
library(gridExtra) # stitching graphs together
library(tree) # fitting the classification tree
library(jtools) # for outputting nice-looking model summaries

# Read in data set
wine <- read.csv2(file = "winequality-red.csv", header = TRUE, dec = ".",
                  colClasses = c(rep("double", 7), rep("double", 4), "integer"))

# Set seed for reproducibility
set.seed(702)
```

\newpage

# Introduction

There has been a push towards the implementation of different analytic solutions within the agriculture industry, especially in the realms of crop health and yield (Byrum et al. 2016). These sort of analyses can help farmers reduce waste and improve profits (Dimitriadis 2008). In particular, the viticulture industry stands to benefit from such methods, being that the production of wine is lengthy and multifaceted. Winemakers can work to optimize revenue by analyzing their input and output (crops and wines) (Santesteban 2019). Specifically, costs may be cut out if machine learning methods were able to predict the quality of wine based on different metrics, since wine producers would no longer have to hire expert wine tasters to determine wine quality. The main question the study looks to answer is can the quality of wine be predicted using a statistical model instead of taste testers. 

The main objective is to model and predict the quality of wine using a variety of statistical learning techniques. This will be done by performing both regression and classification methods, with regression ranking the quality of the wines on a scale of zero to ten, with zero being the lowest quality and ten being the highest quality. The classification models will be classifying the wines into two categories, high and low, with wines in the low category having a score of zero to five, and wines in the high category having a quality score between six and ten. The assessment of the prediction accuracy will be performed by the following methods:


```{r methods}
# Make table of methods
cl <- c("Classification Tree", "Random Forest")
reg <- c("Multiple Linear Regression (MLR)", "Random Forest")
kable(data.frame("Classification" = cl, "Regression" = reg), caption = "ML Methods") %>% 
  row_spec(0, bold=TRUE) %>% collapse_rows(columns = 1, latex_hline = "none") %>% 
  kable_styling(latex_options = "hold_position")
```

The research question of interest for this study is whether or not machine learning methods can accurately predict the quality of a wine, independent of using taste testers. In the classification setting, the team hypothesizes that the machine learning methods can correctly classify wines as "high" or "low" quality with 80% accuracy. In the context of regression, the team hypothesizes that the regression techniques used will be able to achieve a low root mean squared error. In both settings, though, the team expects the random forest to outperform the other methods.

# Required Libraries
To run the code for the project, the following libraries are required:

  * `tidyverse`: for all the data reading and wrangling
  * `knitr`: for rmarkdown table outputs
  * `kableExtra`: for making fancy tables
  * `rmarkdown`: for output documents
  * `corrplot`: for correlation structure visualization
  * `leaps`: for best subsets selection
  * `tree`: fitting the basic classification tree
  * `randomForest`: fitting random forest models
  * `gridExtra`: for stitching plots together
  * `jtools`: for linear model summary output

# Data
The data set used is the Wine Quality data set from the UCI Machine Learning Repository. The data set is comprised of 1600 observations of different variants of the Portuguese "Vinho Verde" red wine. For each wine, various physical and chemical features of each wine were measured, including a measurement of quality given by an expert wine taster. The objective is to use the different features to predict the quality of the wine using multiple machine learning techniques.

## Variable Descriptions  
Each entry in the data set represents the different metrics of the following attributes of a single type of wine.

  * **fixed acidity**: the quantity of fixed acids found in the wines. The predominant fixed acids found in wines are tartaric, malic, citric, and succinic, all of which come from the grapes except for the succinic acid, which comes from the yeast in fermentation. 
  * **volatile acidity**: the steam distillable acids present in wine, primarily acetic acid but also lactic, formic, butyric, and propionic acids. These acids generally come from the fermentation process.
  * **citric acid**: added to the wine as a natural preservative or for acidity and tartness.
  * **residual sugar**: the quantity of sugar left in the wine after the fermentation process.
  * **chlorides**: the amount of salt in a wine.
  * **free sulfur dioxide**: the amount of $SO_2$ that is not bound to other molecules. 
  * **total sulfur dioxide**: total amount of $SO_2$ in the wine. Sulfur Dioxide is used throughout all stages of the winemaking process to prevent oxidation and bacteria growth.
  * **density**: density of the wine.
  * **pH**: pH of the wine.
  * **sulphates**: quantity of sulphates in the wine. Sulphates are used as a preservative.
  * **alcohol**: alcohol content by volume.
  * **quality**: score of quality of the wine given by expert tasters (score between 0 and 10).

## Data Exploration
After reading the data, the first step was to create and view histograms of all of the variables. In doing so, one can get a feel for the distributions of the individual variables.

```{r}
# Create histograms for all 12 variables
hist1 <- ggplot(data = wine, aes(x = fixed.acidity)) + 
           geom_histogram(color = 'black', fill = 'grey', binwidth = 1) +
           labs(x = "Fixed Acidity", y = "Frequency")
hist2 <- ggplot(data = wine, aes(x = volatile.acidity)) + 
           geom_histogram(color = 'black', fill = 'grey', binwidth = .1) +
           labs(x = "Volatile Acidity", y = NULL)
hist3 <- ggplot(data = wine, aes(x = citric.acid)) + 
           geom_histogram(color = 'black', fill = 'grey', binwidth = .075) +
           labs(x = "Citric Acid", y = NULL)
hist4 <- ggplot(data = wine, aes(x = residual.sugar)) + 
           geom_histogram(color = 'black', fill = 'grey', binwidth = 1) +
           labs(x = "Residual Sugar", y = NULL)
hist5 <- ggplot(data = wine, aes(x = chlorides)) + 
           geom_histogram(color = 'black', fill = 'grey', binwidth = .05) +
           labs(x = "Chlorides", y = "Frequency")
hist6 <- ggplot(data = wine, aes(x = free.sulfur.dioxide)) + 
           geom_histogram(color = 'black', fill = 'grey', binwidth = 5) +
           labs(x = "Free Sulfur Dioxide", y = NULL)
hist7 <- ggplot(data = wine, aes(x = total.sulfur.dioxide)) + 
           geom_histogram(color = 'black', fill = 'grey', binwidth = 20) +
           labs(x = "Total Sulfur Dioxide", y = NULL)
hist8 <- ggplot(data = wine, aes(x = density)) + 
           geom_histogram(color = 'black', fill = 'grey', binwidth = .001) +
           labs(x = "Density", y = NULL)
hist9 <- ggplot(data = wine, aes(x = pH)) + 
           geom_histogram(color = 'black', fill = 'grey', binwidth = .1) +
           labs(x = "pH", y = "Frequency")
hist10 <- ggplot(data = wine, aes(x = sulphates)) + 
           geom_histogram(color = 'black', fill = 'grey', binwidth = .1) +
           labs(x = "Sulphates", y = NULL)
hist11 <- ggplot(data = wine, aes(x = alcohol)) + 
           geom_histogram(color = 'black', fill = 'grey', binwidth = .5) +
           labs(x = "Alcohol", y = NULL)
hist12 <- ggplot(data = wine, aes(x = factor(quality))) + 
           geom_bar(color = 'black', fill = 'grey') +
           labs(x = "Quality", y = NULL)
grid.arrange(hist1, hist2, hist3, hist4,
             hist5, hist6, hist7, hist8,
             hist9, hist10, hist11, hist12,
             nrow = 3, ncol = 4)
```

Most wines in the data set seem to be around the 5 to 7 range in terms of quality rating, meaning that there are not very many excellent or very poor wines. The density and pH of the wine seem to be normally distributed, while the sulfur, acid, and sulphate content appear skewed to the right. The density varies very little; looking at the scale on the distribution, one can see that it is approximately normally distributed around 0.997 with a standard deviation of about 0.002. The distribution of alcohol content came as no surprise; an average bottle of wine will range from about 10-12% alcohol by volume, with some exceptions. Beyond 16%, the wine is then considered fortified wine, and red wines don't usually have alcohol content below about 8%.

Next, pairwise scatterplots of the variables were created and evaluated. This method allows one to obtain a greater understanding of how the variables relate to one another, as well as if there is any correlation structure. If the variables are related, there may be problems moving forward, so it is important to know these relationships before the analysis is performed.

```{r pairs}
# Create pairwise scatterplots
pairs(wine[,c(2,5,7,8,9,10,11)])
```

Based on the pairwise scatterplots, one may conclude that pH may be linearly correlated with density, sulphates, and alcohol. This shows that there may be some relationship between the chemical properties of the wines. There also appears to be a relationship between density and alcohol. The following correlation plot gives a numeric summary of the linear relationship between the variables.

```{r corrplot}
# Create visualization of correlation structure
p.mat <- cor.mtest(wine[,c(2,5,7,8,9,10,11)])$p
corrplot(cor(wine[,c(2,5,7,8,9,10,11)]),
         method = "color", type = "upper", order = "hclust", 
         number.cex = .7, addCoef.col = "black",
         tl.col = "black", tl.srt = 90,
         p.mat = p.mat, sig.level = 0.01, insig = "blank", 
         diag = T)
```

The only linear correlations that are notably high are between alcohol and density. This trend is logical because the density of alcohol is slightly less than water, meaning that as the concentration of alcohol gets higher, the density will naturally decrease. Similarly, the sulphates and chlorides seem to have a slightly positive linear relationship, while pH and density have a slightly negative linear relationship.

### Good Wine Qualities v. Bad Wine Qualities

After a split in the data was performed to pull out the wines classified as high quality and the wines classified as low quality, further analysis was performed on each group to see if any of the variables were significantly different between the two groups.  

```{r goodvbad}
good.wine <- subset(wine, quality > 5)
bad.wine <- subset(wine, quality <= 5)
# summary(good.wine)
# summary(bad.wine)

# Express summary output in table form
good.out <- array(rep(0, 12*6), c(12,6))
bad.out <- array(rep(0, 12*6), c(12,6))
for(i in 1:11){
  good.out[i,] <- summary(good.wine[,i]) %>% as.vector()
  bad.out[i,] <- summary(bad.wine[,i]) %>% as.vector()
}
rownames(good.out) <- rownames(bad.out) <- names(good.wine)
colnames(good.out) <- colnames(bad.out) <- c("Min.", "Q1", "Median", "Mean", "Q3", "Max.")

kable(round(good.out, 2), caption = "Summary Output for Good Wines") %>%
  kable_styling(latex_options = "HOLD_position")
kable(round(bad.out, 2), caption = "Summary Output for Bad Wines") %>%
  kable_styling(latex_options = "HOLD_position")
```

The two groups were similar in most of the variables. A notable difference is that the wines classified as high quality had a lower mean value, 39, compared to that of low quality wines, 54.6, for total sulfur dioxide. Another dissimilarity is the difference in the mean for alcohol. The high quality wines had a mean alcohol content at 10.8, while the low quality wines had a lower mean alcohol content around 9.9. Differences also existed in fixed acidity, with the lower quality wines having a lower mean. There were also several variables that do not seem to change between the two groups including pH, residual sugar, chlorides, and density. 

## Data Processing
```{r partition}
# Prepare data for classification
wine$class <- ifelse(wine$quality <= 5, "low", "high") %>% as.factor()

# Create Test and Training Sets
ind <- sample(length(wine$class), 0.8*length(wine$class))

# Create train/test data
trn <- wine[ind,]
tst <- wine[-ind,]
```

Fortunately, the data set was very tidy; there were no missing values. The data was first split into a training and test data set. The training data set uses 80% of the observations and the test set contains the remaining 20%. The models were built on the training set and evaluated on the test set. This is important so that the models are not overfit. A variable was then created for classification, transforming the quality variable into a binary "low" or "high" response as mentioned in the Introduction.

# Classification Models
## Classification Tree
The basic classification tree is based on partitioning the data into subgroups using simple binary splitting. Initially, all observations are considered a single group. Then, the group is repeatedly split into two subgroups based on the criteria of a certain variable. In the classification setting, an observation is classified in a specific region with majority vote. Trees are grown as big as possible and then pruned back using cost-complexity pruning. This is done to ensure the data is not being overfit. The trade-off here is that pruning increases the bias. 

```{r tree}
# Set seed for reprodicibility
set.seed(702)

# 1. full size decision tree
tree.class <- tree(class ~ . - quality, trn)
# plot(tree.class); text(tree.class, pretty = 0)
tree.class.pred <- predict(tree.class, tst, type = 'class')
tree.class.train_err <- mean(trn$class != predict(tree.class, type = 'class'))
tree.class.test_err <- mean(tst$class != tree.class.pred)
# tree.class.train_err
# tree.class.test_err
```

Pruning is performed on the tree using cross validation. A tree is built many times with a different number of splits and the tree that produces the smallest error rate is selected. Cross validation on this model shows that 8 is the optimal number of splits to use, as compared to the first tree which produced 10 splits. The resulting pruned model will now have 8 terminal nodes and uses four variables: Alcohol, Total Sulfur Dioxide, Sulphates, and Volatile Acidity.

```{r prunedtree, out.height = '4in'}
# 2. pruned decision tree 
par(mfrow = c(1,2))
cv.tree.class <- cv.tree(tree.class, FUN = prune.misclass)
plot(cv.tree.class)
prune.tree.class <- prune.misclass(tree.class, best = 8)
plot(prune.tree.class); text(prune.tree.class, pretty = 0, cex = .6)
prune.tree.class.pred <- predict(prune.tree.class, tst, type = 'class')
prune.tree.class.train_err <- mean(trn$class != predict(prune.tree.class, type = 'class'))
prune.tree.class.test_err <- mean(tst$class != prune.tree.class.pred)
tree.res <- c(prune.tree.class.train_err, prune.tree.class.test_err)
# prune.tree.class.test_err
# tree.res
```

The pruned tree produces a test error rate smaller than the original tree with a misclassification error of `r prune.tree.class.test_err`. The advantage of using a basic tree is that it is easy to understand and has a good interpretability, which is not something that translates over to ensemble methods. Additionally, the classification tree is not very computationally expensive, unlike ensemble classification methods. That being said, using ensemble methods like the random forest can often improve the error rate of a simple classification tree, since it is an average of many classification trees. While the test MSE was small in the classification tree, a random forest model was built next, in hopes that it may improve the accuracy of the predictions.

## Random Forest for Classification
The random forest model is an ensemble tree-based method, which creates multiple trees from bootstrap samples and averages the results from all the fitted trees. Many bootstrap samples are created with replacement and then a classification tree is fitted on each bootstrap sample with a random subset of the predictors. Once a prediction has been made by the bootstrapped trees, the final classification is based on majority vote of the bootstrap predictions. Similarly, the prediction in the regression context is an average of the predictions of the bootstrapped trees.  

The advantage for using an ensemble method over a regular classification or regression tree is that because there are many bootstrap samples being averaged together, there is less variance. This is similar to how the variance of the sample mean goes down as the sample size increases. Although it will probably increase our prediction accuracy, the random forest loses the interpretability that the basic trees have. Furthermore, the algorithm that is used to fit the random forest is very computationally expensive.  

The random forest model requires tuning of the number of randomly selected features at each split using cross validation. Here the model is built many times with the number of predictors included from one to all predictors. The test MSE is evaluated for each and the smallest value is selected. A plot of mtry as a function of the misclassification error is included below.

```{r rf classification CV}
#----- classification -----
set.seed(702)
test.err <- double(11)
for (mtry in 1:11){
  fit <- randomForest(class ~ . - quality, trn, mtry = mtry)
  pred <- predict(fit, tst)
  test.err[mtry] <- mean(tst$class != pred)
}

# Plot cross validation results
matplot(1:mtry, test.err, pch = 1, type = 'b',
        xlab = "mtry", ylab = 'Misclassification Error')
points(which.min(test.err), test.err[which.min(test.err)], pch = 8, col = "#CC0000")
```

```{r rfclassmodels}
rf.class <- randomForest(class ~ . - quality, trn, mtry = 2)
rf.class.pred <- predict(rf.class,tst)
rf.class.train_err <- mean(trn$class != predict(rf.class))
rf.class.test_err <- mean(tst$class != rf.class.pred)
rf.class.res <- c(rf.class.train_err, rf.class.test_err)
# rf.class.train_err
# rf.class.test_err
```

From cross validation, the model that produced the smallest error rate was the model built with   `r which.min(test.err)` randomly-chosen variables at each split. This model produced a test error of `r round(rf.class.test_err, 3)`, which is significantly lower than the test error for the basic classification tree (`r round(prune.tree.class.test_err, 3)`). The random forest was selected as the best method for predicting if a wine were to fall into the "high" or "low" quality designation.

# Regression
## Multiple Linear Regression
Multiple linear regression is the extension of simple linear regression to a set of multiple predictor variables. The goal of multiple linear regression is to model the linear relationship between a continuous response and two or more predictor variables. Like simple linear regression, coefficient estimates are found by minimizing the sum of the squared errors. The formula for a multiple linear regression model with $p$ predictor variables is as follows:  

$$
\begin{aligned}
&Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + ... + \beta_pX_{ip} + \epsilon_i, \\
& \quad \text{where observation } i = 1, ..., n \\
& \quad Y_i = \text{dependent variable (response)} \\
& \quad X_{ij} = \text{independent variables for } j = 1, ... , p \\
& \quad \beta_0 = \text{y-intercept} \\
& \quad \beta_j = \text{slope coefficients for each variable} \\
& \quad \epsilon_i = \text{residuals (error term of model)} \\
\end{aligned}
$$
Although it is possible to use every predictor in the MLR model, the method of Best Subsets Selection can help choose the predictors that best explain the variation in the response, for any given model containing a fixed number of predictors. The `leaps` library and the `regsubsets()` function in R produces comparisons of all possible models from the given set of predictors. Best Subset Selection compares all the possible combinations of predictors to select the best model that produces the smallest test error. Three different model criteria were evaluated to compare models of different numbers of predictor variables: Mallow's Cp, BIC, and $R^2_{\text{adj}}$.

```{r reg.select}
# Create model and get summary output
lm.sub <- regsubsets(quality ~ ., data = wine[,-13], nvmax = 11)
smre <- lm.sub %>% summary()

# Plot the Cp, BIC, R^2, RSS results in one plot
par(mfrow = c(2,2))
plot(smre$cp, xlab = "# of Preds", ylab = "Cp")
points(which.min(smre$cp), smre$cp[which.min(smre$cp)], pch = 8, col = "#CC0000")
plot(smre$bic, xlab = "# of Preds", ylab = "BIC")
points(which.min(smre$bic), smre$bic[which.min(smre$bic)], pch = 8, col = "#CC0000")
plot(smre$adjr2, xlab = "# of Preds", ylab = "Adjusted R^2")
points(which.max(smre$adjr2), smre$adjr2[which.max(smre$adjr2)], pch = 8, col = "#CC0000")
#plot(smre$rss, xlab = "# of Preds", ylab = "Training RSS")
#points(which.min(smre$rss), smre$rss[which.min(smre$rss)], pch = 8, col = "#CC0000")

# Best subset selection
best.betas <- coef(lm.sub, id = 6) %>% round(2)
```

Although each of the three model criteria suggested a different number of predictor variables, they were all within the same region (between 6 and 8 predictors). All three produced a similar error rate, so Occam's Razor was implemented and the simplest, six-variable model was selected. This was the model favored by BIC, which gives the greatest penalty for complexity in models. The six variables used in the regression context are `volatile.acidity`, `chlorides`, `total.sulfur.dioxide`, `pH`, `sulphates`, and `alcohol`. All of the variables from the classification selection appear with the exception of density. In the regression context, `pH` was the variable that proved to be a rather unintuitive inclusion, because it is similar to density in that it is not easily detected by human senses. The inclusion of these two variables in the model portray the importance of the chemical properties of a bottle of wine when it comes to quality.

```{r mlr}
#### REGRESSION ####
lm.reg <- lm(quality ~ volatile.acidity + chlorides + 
               pH + total.sulfur.dioxide + sulphates +
               alcohol, data = trn)
summ(lm.reg, digits = 3)

# Obtain RMSE and results for linear model
lm.reg.trn.pred <- predict(lm.reg, newdata = trn)
lm.reg.trn.rmse <- sqrt(mean((lm.reg.trn.pred - trn$quality)^2))

lm.reg.pred <- predict(lm.reg, newdata = tst)
lm.reg.rmse <- sqrt(mean((lm.reg.pred - tst$quality)^2))
lm.res <- c(lm.reg.trn.rmse, lm.reg.rmse)
```

The adjusted R-squared value 0.363 suggests that these six predictors combined explain about 36.3% of the variance in `quality` of a wine.

The multiple linear regression model is subject to the following assumptions about the data:  

  * Linear relationship between response and predictors
  * No collinearity exists between predictor variables
  * $Y_i$'s are independent and identically distributed
  * $\epsilon_i \sim N(0, \sigma^2)$

To assess these assumptions, various diagnostics plots were evaluated:
```{r lm.diag}
# Look at residual plot for nonlinearity
regres <- ggplot(data = data.frame(lm.reg$residuals, lm.reg$fitted.values), 
                 aes(y = lm.reg$residuals, x = lm.reg$fitted)) + 
            geom_point(position = 'jitter') + labs(y = "Residuals", x = "Fitted Values") 

# Look at standardized residual plot for outliers
stdres <- ggplot(data = data.frame(x = 1:length(lm.reg$residuals), 
                                   y = scale(lm.reg$residuals)), 
                 aes(y = y, x = x)) + 
            geom_point() + labs(x = "Index", y = "Standardized Residuals")

# Look at a qqplot to assess normality
ggqq <- ggplot(data.frame(lm.reg$residuals), aes(sample = lm.reg$residuals)) + 
          stat_qq() + stat_qq_line(col = "blue", lty = 2) + 
          labs(x = "Theoretical", y = "Sample")

# Look at residuals vs leverage plot
reslev <- ggplot(lm.reg, aes(x = .hat, y = .stdresid)) + 
            geom_point(aes(size = .cooksd)) + 
            stat_smooth(method = "lm") + 
            labs( x = "Leverage", y = "Standardized Residuals") + 
            scale_size_continuous("Cook's D", range = c(1,5)) +
            theme_bw() + theme(legend.position = "right")

# Stitch all the graphs together
grid.arrange(regres, stdres, ggqq, reslev, nrow = 2)

scale.res <- scale(lm.reg$residuals)
scale.res.2std <- mean((scale.res <= 1.96) * (scale.res >= -1.96))
```

Residuals indicate that the MLR assumptions are mostly satisfied. The residuals vs. fitted values plot (top left) has unusual stratified lines, but this effect is caused by the observed responses being integer values of 4, 5, 6, 7, etc. rather than the continuous values typical of a regression. The plot does not show heteroscedasticity and residuals are spread evenly around 0 over the entire range of fitted values. When residuals are standardized, as in the top right plot, one would expect about 95% to be inside the [-1.96, 1.96] interval; the actual percentage is `r round(100*scale.res.2std, 1)`%. The QQ-plot in the bottom left compares the normality of residuals against a theoretical distribution. Except for a slight downward bias of the lowest 5% of residuals at 2 standard deviations below 0, the data fits a normal distribution almost perfectly. On the bottom right plot, high leverage points (those with leverage $\ge$ 0.03) are not dramatically biased, being spread both above and below the 0 residual. Cook's Distance tops out at about 0.08, well below the generally accepted threshold of 1 that is considered highly influential. These plots taken together suggest that the data generally fulfills the requirements for modeling using MLR.

The largest benefit to using a multiple linear regression model is its interpretability and ease of use. The coefficients can be computed efficiently and they have clear and easy interpretations. It is also possible to make inference on any of the variables or linear combination of variables. The most significant drawback of using the multiple linear regression model is that the model is subject to distributional assumptions, meaning that if the assumptions are violated, the estimates and inference will be unstable at best and invalid at worst. But based on the residual plots it appears that the model does not violate any of the model assumptions.

## Random Forest for Regression
As mentioned earlier in the Classification section, Random Forest is an ensemble tree-based method, which creates multiple trees from bootstrap samples and averages the results from all the fitted trees. This model will be built exactly in the same way as the classification model, except the tree will not be predicting if the wine is a part of the "high" or "low" class, but rather predicts the quality score associated with each wine. The predicted value of the quality score will be an average of all of the generated regression trees. As before, the model will first be built using cross validation to determine the number of variables that should be used in order to produce the smallest test RMSE.

```{r rf.reg}
#----- regression -----
# loop thru all the different values of mtry
test.err = double(11)
for (mtry in 1:11){
  fit<- randomForest(quality ~ . - class, trn, mtry = mtry)
  pred <- predict(fit, tst)
  test.err[mtry] <- sqrt(mean((tst$quality - pred)^2))
}

# Plot cross validation results
matplot(1:mtry, test.err, pch = 1, type = 'b', ylab = 'Root MSE', xlab = "mtry")
points(which.min(test.err), test.err[which.min(test.err)], pch = 8, col = "#CC0000")

# fit the random forest with the best results
rf.reg <- randomForest(quality ~ . - class, trn, mtry = 2)
rf.reg.pred <- predict(rf.reg, tst)
rf.reg.train_err <- sqrt(mean((trn$quality - predict(rf.reg))^2))
rf.reg.test_err <- sqrt(mean((tst$quality - rf.reg.pred)^2))
rf.reg.res <- c(rf.reg.train_err, rf.reg.test_err)
```  

Based on cross validation, the random forest model for regression using two randomly-selected variables considered at each split was selected. This model produced a test RMSE of `r round(rf.reg.test_err,4)`.  

# Results  

```{r create.df}
# Create data frame output for regression models
df.reg <- rbind.data.frame(lm.res, rf.reg.res) %>% round(4)
rownames(df.reg) <- c("Multiple Linear Regression", "Random Forest")
colnames(df.reg) <- c("Training RMSE", "Test RMSE")

# Create data frame output for classification models
df.class <- rbind.data.frame(tree.res, rf.class.res) %>% round(4)
colnames(df.class) <- c("Training Error", "Test Error")
rownames(df.class) <- c("Classification Tree", "Random Forest")
```

```{r reg.out}
# Smash tables next to each other using kable
kable(df.reg, caption = "Regression Model Results", format = "latex") %>% 
  row_spec(0, bold=TRUE) %>% collapse_rows(columns = 1, latex_hline = "none") %>%
  kable_styling(latex_options = "HOLD_position")
```

```{r class.out}
kable(df.class, caption = "Classification Model Results", format = "latex") %>% 
  row_spec(0, bold=TRUE) %>% collapse_rows(columns = 1, latex_hline = "none") %>%
  kable_styling(latex_options = "HOLD_position")
```

Based on the output, the best model for both classification and regression was the random forest. This supports the hypothesis that the random forest would outperform the other methods. This was not necessarily a surprise, since the ensemble methods tend to outperform regular methods. As an average of many classification trees, ensemble methods generally improve prediction accuracy and reduce overall variance. Other ensemble methods, such as bagging or boosting, may also be able to yield higher prediction accuracy.   

In the regression setting, the test root mean squared error (RMSE) for both models were relatively low. The RMSE for the random forest was `r df.reg[2,2]` and the RMSE for the multiple linear regression model was `r df.reg[1,2]`, further supporting the hypothesis that the random forest would outperform the other models.

One of the main disadvantages to using ensemble techniques like the random forest is the fact that they tend to be very computationally intensive. The basic classification tree and multiple linear regression model took significantly less time to run than the random forest, but they were less accurate in their predictions. If an ensemble method were run on a very large data set with a lot of features, the model would take much longer to run and would be incredibly difficult to interpret.  

# Conclusion  
With the recent push for quantitative solutions in industries like agriculture, there has been an emphasis on using machine learning to solve a variety of problems requiring prediction and inference. In particular, the viticulture industry stands to gain a lot from such methods, since historically it has not interacted much with the world of analytics. In this study, various machine learning techniques were performed that assessed the prediction accuracy in both classification and regression contexts.

Based on the results of the study, prediction of if a given wine has high or low quality was achieved with about 70-80% accuracy. Despite not being able to achieve 80% prediction accuracy with the basic classification tree, the random forest model showed an improvement and predicted with a higher accuracy. Although this is a *good* result considering the no-information rate of 53%, it is by no means excellent. These results indicate that machine learning algorithms could be a good technique when trying to predict wine quality, but that there is room for improvement. With a more formal, industry-expert-guided variable selection process, the team believes that higher prediction accuracy can be achieved. Machine learning models could potentially be combined with other techniques to add value to winemakers.

## Limitations  
The multiple linear regression model is subject to certain distributional and model assumptions, which were not explicitly tested or checked. Since the objective of this project was to predict rather than make inference, model assumptions were not explicitly checked.

All wines in the data set came from wineries in Portugal, although they are from different wineries, one may worry that the results may not be generalizable to wine production globally. Similarly, will these prediction results hold when it comes to white wine, rose, or sparkling wine? Given this variability over location and type of wine, what can be said about the wine-making industry as a whole?

Some of the wine regions have more strict standards for the designations of wine quality. For example, regions like Bordeaux and Champagne in France have some of the highest standards in the world for their wines, which are much different than somewhere like Greece or New Zealand. How accurate will the predictions be when there are different standards and how might one account for that variation?

## Further Research  
Since this study solely looks at the prediction accuracy of machine learning techniques for red wines, the natural extension of the analysis would be to verify that the results hold constant for white wines as well. The University of California, Irvine Machine Learning database has data on white wines, so that would be a good place to start.

The characteristics of any given wine are often derived from different geological and/or climatological
conditions. For example, a wine that comes from a region close enough to the water to get a sea breeze may have a hint of saltiness in its aroma or taste. Similarly, a grape coming from a relatively young grove might give the wine different features than if the grove were hundreds of years old. It would be interesting to see how different weather conditions, terroirs, plant ages, and slopes affect the outcome of the wine in terms of quality. 

# References

  * Byrum, Joseph, et al. "Advanced analytics for agricultural product development." Interfaces 46.1 (2016): 5-17.
  * Dimitriadis, Savvas, and Christos Goumopoulos. "Applying machine learning to extract new knowledge in precision agriculture applications." 2008 Panhellenic Conference on Informatics. IEEE, 2008.
  * Liakos, Konstantinos G., et al. "Machine learning in agriculture: A review." Sensors 18.8 (2018): 2674.
  * Santesteban, Luis G. "Precision viticulture and advanced analytics. A short review." Food chemistry 279 (2019): 58-62.
